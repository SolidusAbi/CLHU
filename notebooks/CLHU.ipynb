{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_root_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_root_dir not in sys.path:\n",
    "    sys.path.append(project_root_dir)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import config\n",
    "\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.metrics import UnmixingLoss, NormalizedEntropy\n",
    "from HySpecLab.metrics.regularization import SimplexVolumeLoss, SimilarityLoss\n",
    "\n",
    "from HySpecLab.unmixing import ContrastiveUnmixing\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def train(model:nn.Module, n_endmembers:int, dataset:Dataset, n_batchs:int = 64, n_epochs:int = 100, lr=1e-3, simplex_weight=1e-5):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = UnmixingLoss() \n",
    "    volume_reg = SimplexVolumeLoss(dataset[:], n_endmembers).to(device)\n",
    "    similarity_reg = SimilarityLoss(n_endmembers, temperature=.1, reduction='mean')\n",
    "    vol_reg_norm = volume_reg(torch.sigmoid(model.ebk.detach()))\n",
    "    print(vol_reg_norm)\n",
    "    \n",
    "\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=int(len(dataset)/n_batchs), shuffle=True)\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % -1},\n",
    "        )\n",
    "\n",
    "    similarity_weight = 1e-1\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        epoch_loss = 0.\n",
    "        for i, (x) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y = model(x)\n",
    "            loss = criterion(y, x) + simplex_weight*(volume_reg(sigmoid(model.ebk))/vol_reg_norm) + similarity_weight*similarity_reg(model.ebk)\n",
    "            epoch_loss += loss.detach().item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        epoch_iterator.set_postfix(tls=\"%.4f\" % (epoch_loss/(i+1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samson_save_path = os.path.join(config.IMG_PATH, 'Samson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Samson\n",
    "dataset = Samson(config.Samson_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.eea import VCA\n",
    "n_endmembers = dataset.n_endmembers + 0\n",
    "\n",
    "vca = VCA(n_endmembers=n_endmembers, snr_input=1, random_state=42)\n",
    "\n",
    "E = vca.fit(dataset.X.numpy())\n",
    "endmember_init = torch.from_numpy(vca.endmembers()).float()\n",
    "# forces that the max value of each ealemend is 1 - 1e-3, For testing!!\n",
    "# endmember_init = (endmember_init / endmember_init.max(dim=1, keepdim=True)[0]) * .9\n",
    "logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_endmembers\n",
    "fig = plot_endmembers(endmember_init)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysptools import eea\n",
    "n_endmembers = dataset.n_endmembers\n",
    "\n",
    "ee = eea.NFINDR()\n",
    "endmember = ee.extract(dataset.image().numpy(), n_endmembers)\n",
    "endmember_init = torch.from_numpy(endmember).float()\n",
    "logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))\n",
    "\n",
    "\n",
    "fig = plot_endmembers(endmember_init)\n",
    "plt.show(fig)\n",
    "# with plt.style.context((\"seaborn-colorblind\")):\n",
    "#     plt.plot(endmember_init.T)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_endmembers = dataset.n_endmembers + 0\n",
    "\n",
    "# ee = eea.FIPPI()\n",
    "# endmember = ee.extract(dataset.image().numpy(), n_endmembers-1)\n",
    "# endmember_init = torch.from_numpy(endmember).float()\n",
    "# endmember_init = endmember_init[1:]\n",
    "# logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))\n",
    "\n",
    "# with plt.style.context((\"seaborn-colorblind\")):\n",
    "#     plt.plot(endmember_init.T)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = dataset.n_bands\n",
    "model = ContrastiveUnmixing(n_bands, n_endmembers, endmember_init=logit_endmember_init)\n",
    "# train(model, n_endmembers, dataset, n_batchs=32, n_epochs=100, lr=1e-3)\n",
    "train(model, n_endmembers, dataset, n_batchs=50, n_epochs=50, lr=1e-3, simplex_weight=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.metrics import NormalizedEntropy\n",
    "\n",
    "criterion = UnmixingLoss()\n",
    "entropy_reg  = NormalizedEntropy(S=n_endmembers)\n",
    "volume_reg = SimplexVolumeLoss(dataset[:], n_endmembers)\n",
    "similarity_reg = SimilarityLoss(n_endmembers, temperature=.1, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = dataset.X\n",
    "\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "reconstruc = model(_X)\n",
    "with torch.no_grad():\n",
    "    print(criterion(reconstruc, _X).cpu(), entropy_reg(model.A).cpu(), volume_reg(sigmoid(model.ebk)).cpu(),\n",
    "         similarity_reg(sigmoid(model.ebk)).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_reg(endmember_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebk = sigmoid(model.ebk).detach().cpu()\n",
    "fig = plot_endmembers(ebk)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering the endmembers\n",
    "endmembers = dataset.endmembers().detach().cpu()\n",
    "from HySpecLab.metrics import sad\n",
    "sad_result = sad(ebk, endmembers)\n",
    "print(sad_result)\n",
    "idx = torch.argmin(sad_result, dim=1) # Index for reordering the ground truth\n",
    "print(idx)\n",
    "\n",
    "# reorder the endmembers\n",
    "endmembers = endmembers[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering the endmembers\n",
    "endmembers = dataset.endmembers().detach().cpu()\n",
    "from HySpecLab.metrics import sad\n",
    "sad_result = sad(endmember_init, endmembers)\n",
    "print(sad_result)\n",
    "idx = torch.argmin(sad_result, dim=1) # Index for reordering the ground truth\n",
    "print(idx)\n",
    "\n",
    "# reorder the endmembers\n",
    "endmembers = endmembers[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_endmembers(endmembers)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "test = softmax(model.A.detach(), dim=1).cpu().numpy().reshape(dataset.n_row, dataset.n_col, n_endmembers)\n",
    "labels = list(map(lambda x: f'$E_{x}$', range(1, n_endmembers+1)))\n",
    "\n",
    "# with plt.style.context((\"seaborn-colorblind\")):\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "for i in range(n_endmembers):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    # plt.imshow(test[:,:,i].T, vmin=0, vmax=softmax(model.A, dim=1).max(), cmap='viridis')\n",
    "    plt.imshow(test[:,:,i].T, cmap='viridis')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(labels[i], fontsize='x-large')\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebk = sigmoid(model.ebk).detach().cpu()\n",
    "endmembers = dataset.endmembers()\n",
    "\n",
    "from torch.nn.functional import mse_loss\n",
    "def rmse(x: torch.Tensor, y: torch.Tensor):\n",
    "    return torch.sqrt(mse_loss(x, y, reduction='none').mean(dim=1))\n",
    "\n",
    "abundance = softmax(model.A.detach(), dim=1).cpu().reshape(dataset.n_row, dataset.n_col, n_endmembers)\n",
    "abundance = abundance.permute(2,0,1)\n",
    "abundance_gt = dataset.abundance()[:,:,idx].permute(2,0,1) # Reorder the ground truth\n",
    "endmember_gt = dataset.endmembers()[idx, :]\n",
    "\n",
    "rmse_result = rmse(abundance.flatten(1), abundance_gt.flatten(1))\n",
    "print(rmse_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7))\n",
    "for i in range(n_endmembers):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    # plt.imshow(test[:,:,i].T, vmin=0, vmax=softmax(model.A, dim=1).max(), cmap='viridis')\n",
    "    plt.imshow(abundance_gt[i,:,:].T, cmap='viridis')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(labels[i], fontsize='x-large')\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Urban\n",
    "dataset = Urban(root_dir=config.Urban_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.eea import VCA\n",
    "n_endmembers = dataset.n_endmembers\n",
    "endmembers = dataset.endmembers()\n",
    "\n",
    "# from HySpecLab.metrics import sad\n",
    "# import numpy as np\n",
    "# for i in range(256):\n",
    "#     vca = VCA(n_endmembers=n_endmembers, snr_input=1, random_state=i)\n",
    "\n",
    "#     E = vca.fit(dataset.X.numpy())\n",
    "#     endmember_init = torch.from_numpy(vca.endmembers()).float()\n",
    "\n",
    "#     sad_result = sad(endmember_init, endmembers)\n",
    "#     idx = torch.argmin(sad_result, dim=1) # Index for reordering the ground truth\n",
    "#     if np.unique(idx).shape[0] == n_endmembers:\n",
    "#         print(i)\n",
    "#         break\n",
    "\n",
    "vca = VCA(n_endmembers=n_endmembers, snr_input=1, random_state=42)\n",
    "\n",
    "E = vca.fit(dataset.X.numpy())\n",
    "endmember_init = torch.from_numpy(vca.endmembers()).float()\n",
    "\n",
    "# forces that the max value of each ealemend is 1 - 1e-3, For testing!!\n",
    "# endmember_init = (endmember_init / endmember_init.max(dim=1, keepdim=True)[0]) * .9\n",
    "logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))\n",
    "\n",
    "\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    plt.plot(endmember_init.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysptools import eea\n",
    "n_endmembers = dataset.n_endmembers\n",
    "\n",
    "ee = eea.NFINDR()\n",
    "endmember = ee.extract(dataset.image().numpy(), n_endmembers)\n",
    "endmember_init = torch.from_numpy(endmember).float()\n",
    "logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))\n",
    "\n",
    "\n",
    "fig = plot_endmembers(endmember_init)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = dataset.n_bands\n",
    "model = ContrastiveUnmixing(n_bands, n_endmembers, endmember_init=logit_endmember_init)\n",
    "# train(model, n_endmembers, dataset, n_batchs=32, n_epochs=100, lr=1e-3)\n",
    "train(model, n_endmembers, dataset, n_batchs=50, n_epochs=50, lr=1e-3, simplex_weight=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = UnmixingLoss()\n",
    "entropy_reg  = NormalizedEntropy(S=n_endmembers)\n",
    "volume_reg = SimplexVolumeLoss(dataset[:], n_endmembers)\n",
    "similarity_reg = SimilarityLoss(n_endmembers, temperature=.1, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = dataset.X\n",
    "\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "reconstruc = model(_X)\n",
    "with torch.no_grad():\n",
    "    print(criterion(reconstruc, _X).cpu(), entropy_reg(model.A).cpu(), volume_reg(sigmoid(model.ebk)).cpu(),\n",
    "         similarity_reg(sigmoid(model.ebk)).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_reg(endmember_init), volume_reg(endmember_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(map(lambda x: f'$E_{x}$', range(1, n_endmembers+1)))\n",
    "ebk = sigmoid(model.ebk).detach().cpu()\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    plt.plot(ebk.T, label=labels)\n",
    "    plt.ylabel('Reflectance', fontsize='x-large')\n",
    "    plt.xlabel('Bands', fontsize='x-large')\n",
    "    #legend background white\n",
    "    plt.legend(fontsize='x-large')\n",
    "    plt.xticks(fontsize='x-large')\n",
    "    plt.yticks(fontsize='x-large')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering the endmembers\n",
    "endmembers = dataset.endmembers().detach().cpu()\n",
    "from HySpecLab.metrics import sad\n",
    "sad_result = sad(ebk, endmembers)\n",
    "print(sad_result)\n",
    "idx = torch.argmin(sad_result, dim=1) # Index for reordering the ground truth\n",
    "print(idx)\n",
    "\n",
    "# idx[1] = 1\n",
    "idx[-1] = 1\n",
    "# idx[-2] = 1\n",
    "\n",
    "# reorder the endmembers\n",
    "endmembers = endmembers[idx]\n",
    "print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(map(lambda x: f'$E_{x}$', range(1, len(dataset.endmembers())+1)))\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    plt.plot(dataset.endmembers().T, label=labels)\n",
    "    plt.ylabel('Reflectance', fontsize='x-large')\n",
    "    plt.xlabel('Bands', fontsize='x-large')\n",
    "    plt.legend(fontsize='x-large', facecolor='white')\n",
    "    plt.xticks(fontsize='x-large')\n",
    "    plt.yticks(fontsize='x-large')\n",
    "    # plt.title('Ground Truth', fontsize='x-large')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "test = softmax(model.A.detach(), dim=1).cpu().numpy().reshape(dataset.n_row, dataset.n_col, n_endmembers)\n",
    "labels = list(map(lambda x: f'$E_{x}$', range(1, n_endmembers+1)))\n",
    "\n",
    "# with plt.style.context((\"seaborn-colorblind\")):\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "for i in range(n_endmembers):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    # plt.imshow(test[:,:,i].T, vmin=0, vmax=softmax(model.A, dim=1).max(), cmap='viridis')\n",
    "    plt.imshow(test[:,:,i].T, cmap='viridis')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(labels[i], fontsize='x-large')\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebk = sigmoid(model.ebk).detach().cpu()\n",
    "endmembers = dataset.endmembers()\n",
    "print(endmembers.shape)\n",
    "\n",
    "from torch.nn.functional import mse_loss\n",
    "def rmse(x: torch.Tensor, y: torch.Tensor):\n",
    "    return torch.sqrt(mse_loss(x, y, reduction='none').mean(dim=1))\n",
    "\n",
    "abundance = softmax(model.A.detach(), dim=1).cpu().reshape(dataset.n_row, dataset.n_col, n_endmembers)\n",
    "abundance = abundance.permute(2,0,1)\n",
    "abundance_gt = dataset.abundance()[:,:,idx].permute(2,0,1) # Reorder the ground truth\n",
    "print(abundance_gt.shape)\n",
    "endmember_gt = dataset.endmembers()[idx, :]\n",
    "\n",
    "rmse_result = rmse(abundance.flatten(1), abundance_gt.flatten(1))\n",
    "print(rmse_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7))\n",
    "for i in range(n_endmembers):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    # plt.imshow(test[:,:,i].T, vmin=0, vmax=softmax(model.A, dim=1).max(), cmap='viridis')\n",
    "    plt.imshow(abundance_gt[i,:,:].T, cmap='viridis')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(labels[i], fontsize='x-large')\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuprite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuprite_save_path = os.path.join(config.IMG_PATH, 'Cuprite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Cuprite\n",
    "dataset = Cuprite(config.Cuprite_PATH)\n",
    "\n",
    "plt.imshow(dataset.image()[:,:,0], cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.eea import VCA\n",
    "# n_endmembers = dataset.n_endmembers\n",
    "n_endmembers = 12\n",
    "\n",
    "vca = VCA(n_endmembers=n_endmembers, snr_input=1, random_state=1024)\n",
    "\n",
    "E = vca.fit(dataset.X.numpy())\n",
    "endmember_init = torch.from_numpy(vca.endmembers()).float()\n",
    "\n",
    "# forces that the max value of each ealemend is 1 - 1e-3, For testing!!\n",
    "# endmember_init = (endmember_init / endmember_init.max(dim=1, keepdim=True)[0]) * .9\n",
    "logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))\n",
    "\n",
    "from utils import plot_endmembers\n",
    "import numpy as np\n",
    "def plot_endmembers(E: np.ndarray, wv:np.ndarray = None, labels:list = None, figsize:tuple = (7,5), ticks_range:tuple=(0, 1), n_ticks:int=5):\n",
    "    '''\n",
    "        Plot endmembers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            E : 2-D array, shape (n_endmembers, n_bands)\n",
    "                Endmembers.\n",
    "            wv : 1-D array, optional, shape (n_bands)\n",
    "                Wavelengths in nm. Default is None.\n",
    "            labels : list, optional\n",
    "                Labels for endmembers. Default is None.\n",
    "            figsize : tuple, optional\n",
    "                Figure size. Default is (7,5).\n",
    "            ticks_range : tuple, optional\n",
    "                Range of yticks. Default is (0, 1).\n",
    "            n_ticks : int, optional\n",
    "                Number of yticks. Default is 5.\n",
    "    '''\n",
    "    ticks_formatter = plt.FormatStrFormatter('%.2f')\n",
    "\n",
    "    n_endmembers, n_bands = E.shape\n",
    "    if labels is None:\n",
    "        labels = list(map(lambda x: r'$E_{{{}}}$'.format(x), range(1, n_endmembers+1)))\n",
    "\n",
    "    with plt.style.context((\"seaborn-colorblind\")):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "        ticks = np.linspace(*ticks_range, n_ticks)\n",
    "        if wv is None:\n",
    "            ax.plot(E.T, label=labels)\n",
    "            ax.set_xlabel('Bands', fontsize='x-large')\n",
    "        else:\n",
    "            ax.plot(wv, E.T, label=labels)\n",
    "            ax.set_xlabel('Wavelength (nm)', fontsize='x-large')\n",
    "\n",
    "        ax.set_ylabel('Reflectance', fontsize='x-large')           \n",
    "        ax.set_yticks(ticks)\n",
    "        ax.yaxis.set_major_formatter(ticks_formatter) # set format in y ticks labels\n",
    "        ax.set_ylim(ticks_range[0] - 0.025, ticks_range[1] + 0.025)\n",
    "        ax.set_xlim(0 - 1.5, n_bands + 1.5)\n",
    "        ax.tick_params(axis='both', labelsize='large')\n",
    "    \n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc='upper center', ncol=6, fontsize='large', borderpad=-.25)\n",
    "        fig.tight_layout(pad=(((n_endmembers-1)//6)+1)*2) # padding based on the endmembers number\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_endmembers(endmember_init[:12], ticks_range=(0, endmember_init.max()))\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysptools import eea\n",
    "n_endmembers = dataset.n_endmembers\n",
    "# n_endmembers = 3\n",
    "\n",
    "ee = eea.NFINDR()\n",
    "endmember = ee.extract(dataset.image().numpy(), n_endmembers)\n",
    "endmember_init = torch.from_numpy(endmember).float()\n",
    "logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))\n",
    "\n",
    "# from utils import plot_endmembers\n",
    "fig = plot_endmembers(endmember_init)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = UnmixingLoss()\n",
    "entropy_reg  = NormalizedEntropy(S=n_endmembers)\n",
    "volume_reg = SimplexVolumeLoss(dataset[:], n_endmembers)\n",
    "similarity_reg = SimilarityLoss(n_endmembers, temperature=.1, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_reg(endmember_init), similarity_reg(endmember_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = dataset.n_bands\n",
    "model = ContrastiveUnmixing(n_bands, n_endmembers, endmember_init=logit_endmember_init)\n",
    "# train(model, n_endmembers, dataset, n_batchs=32, n_epochs=100, lr=1e-3)\n",
    "train(model, n_endmembers, dataset, n_batchs=50, n_epochs=50, lr=1e-3, simplex_weight=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = UnmixingLoss()\n",
    "entropy_reg  = NormalizedEntropy(S=n_endmembers)\n",
    "volume_reg = SimplexVolumeLoss(dataset[:], n_endmembers)\n",
    "similarity_reg = SimilarityLoss(n_endmembers, temperature=.1, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = dataset.X\n",
    "\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "reconstruc = model(_X)\n",
    "with torch.no_grad():\n",
    "    print(criterion(reconstruc, _X).cpu(), entropy_reg(model.A).cpu(), volume_reg(sigmoid(model.ebk)).cpu(),\n",
    "         similarity_reg(sigmoid(model.ebk)).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_reg(endmember_init), similarity_reg(endmember_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebk = sigmoid(model.ebk).detach().cpu()\n",
    "\n",
    "fig = plot_endmembers(ebk)\n",
    "plt.show(fig)\n",
    "\n",
    "fig = plot_endmembers(dataset.endmembers())\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "test = softmax(model.A.detach(), dim=1).cpu().numpy().reshape(dataset.n_row, dataset.n_col, n_endmembers)\n",
    "\n",
    "def show_abundance(A, labels:list = None, figsize:tuple=(7,5)):\n",
    "    n_rows, n_cols, n_endmembers = A.shape\n",
    "\n",
    "    if labels is None:\n",
    "        labels = list(map(lambda x: r'$E_{{{}}}$'.format(x), range(1, n_endmembers+1)))\n",
    "        \n",
    "    ticks_formatter = plt.FormatStrFormatter('%.1f')\n",
    "    fig = plt.figure(figsize=(7,5))\n",
    "    for i in range(n_endmembers):\n",
    "        data = A[:,:,i].T\n",
    "        plt.subplot(3,4,i+1)\n",
    "        plt.imshow(data, cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(labels[i], fontsize='x-large')\n",
    "        cb = plt.colorbar(format=ticks_formatter, ticks=[data.min() + 1e-3, data.max() - 1e-3],\n",
    "                         orientation='horizontal', fraction=0.1, pad=0.01)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = show_abundance(test)\n",
    "fig.savefig('abundance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_root_dir = os.path.join(os.getcwd(),'../..')\n",
    "if project_root_dir not in sys.path:\n",
    "    sys.path.append(project_root_dir)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "import config\n",
    "\n",
    "from utils import show_abundance, plot_endmembers\n",
    "from dataset import JasperRidgeDataset\n",
    "from HySpecLab.metrics import rmse, sad\n",
    "from scipy import io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endmembers = dataset.endmembers()\n",
    "ebk = sigmoid(model.ebk).detach().cpu()\n",
    "\n",
    "from HySpecLab.metrics import sad\n",
    "sad_result = sad(ebk, endmembers)\n",
    "print(sad_result)\n",
    "import numpy as np\n",
    "idx = np.argmin(sad_result, axis=1)\n",
    "print(idx)\n",
    "\n",
    "print(np.unique(idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchStay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41f72df89919d74d3a449896230e7539fdf54ec0bbe47e8a03d934e2ec4dfa40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
