{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_root_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_root_dir not in sys.path:\n",
    "    sys.path.append(project_root_dir)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import config\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas\n",
    "\n",
    "Gradient of EBK based exclusively to the reconstruction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.metrics import UnmixingLoss, NormalizedEntropy\n",
    "from HySpecLab.metrics.regularization import SimplexVolumeLoss, SimilarityLoss\n",
    "\n",
    "from HySpecLab.unmixing import ContrastiveUnmixing\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def train(model:nn.Module, n_endmembers:int, dataset:Dataset, n_batchs:int = 64, n_epochs:int = 100, lr=1e-3):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = UnmixingLoss()\n",
    "    entropy_reg  = NormalizedEntropy(S=n_endmembers)    \n",
    "    volume_reg = SimplexVolumeLoss(dataset[:], n_endmembers).to(device)\n",
    "    similarity_reg = SimilarityLoss(n_endmembers, temperature=.1, reduction='mean')\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=int(len(dataset)/n_batchs), shuffle=True)\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % -1},\n",
    "        )\n",
    "\n",
    "    entropy_weight = 1e-1\n",
    "    simplex_weight = 1e-4\n",
    "    similarity_weight = 1e-1\n",
    "\n",
    "    # entropy_weight = 0\n",
    "    # simplex_weight = 0\n",
    "    # similarity_weight = 0\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        epoch_loss = 0.\n",
    "        for i, (x) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y = model(x)\n",
    "            loss = criterion(y, x) + entropy_weight*entropy_reg(model.A) + simplex_weight*volume_reg(sigmoid(model.ebk)) + similarity_weight*similarity_reg(model.ebk)\n",
    "            epoch_loss += loss.detach().item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        epoch_iterator.set_postfix(tls=\"%.4f\" % (epoch_loss/(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch import tensor\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "class HSIDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super(HSIDataset, self).__init__()\n",
    "\n",
    "    def preprocessing(self, X:np.ndarray):\n",
    "        '''\n",
    "            Preprocessing the dataset for removing high-frequency noise. \n",
    "            This preprocessing consists of three steps:\n",
    "                1. Median filter in the spatial domain.\n",
    "                2. Moving average filter in the spectral domain.\n",
    "                3. Normalization of the data.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "                X : np.ndarray, shape (nRow, nCol, nBand)\n",
    "                    HSI Cube.\n",
    "        '''\n",
    "\n",
    "        from skimage.filters import median\n",
    "        from utils import moving_average\n",
    "\n",
    "        max_value = X.max()\n",
    "\n",
    "        X = median(X, footprint=np.ones((3,3,1)))\n",
    "        # X = moving_average(X.reshape(-1, X.shape[-1]), 3, padding_size=2).reshape(X.shape[0], X.shape[1], -1)\n",
    "        X = X / (max_value + 1e-3)\n",
    "        return X\n",
    "\n",
    "class JasperRidge(HSIDataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super(JasperRidge, self).__init__()\n",
    "        data = sio.loadmat(os.path.join(root_dir, 'jasperRidge2_R198.mat'))\n",
    "        y = sio.loadmat(os.path.join(root_dir, 'GroundTruth/end4.mat'))\n",
    "\n",
    "        self.n_row, self.n_col = data['nRow'].item(), data['nCol'].item()\n",
    "\n",
    "        self.X = data['Y'].T.reshape(self.n_row, self.n_col, -1) # (nRow, nCol, nBand)\n",
    "        self.X = self.preprocessing(self.X).reshape(-1, self.X.shape[-1]) # (nRow*nCol, nBand)\n",
    "        self.X = tensor(self.X, dtype=torch.float32)\n",
    "\n",
    "        self.E = tensor(y['M'].T, dtype=torch.float32) # (nEndmember, nBand)\n",
    "        self.A = tensor(y['A'].T, dtype=torch.float32) # (nRow*nCol, nEndmember)\n",
    "        self.n_bands = self.X.shape[1]\n",
    "        self.n_endmembers = self.E.shape[0]\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_row * self.n_col\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def endmembers(self):\n",
    "        return self.E\n",
    "\n",
    "    def abundance(self):\n",
    "        return self.A.reshape(self.n_row, self.n_col, -1)\n",
    "\n",
    "    def image(self):\n",
    "        return self.X.reshape(self.n_row, self.n_col, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'JasperRidge'\n",
    "dataset = JasperRidge(config.JasperRidge_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.eea import VCA\n",
    "\n",
    "n_endmembers = dataset.n_endmembers + 0\n",
    "vca = VCA(n_endmembers=n_endmembers, snr_input=1, random_state=128)\n",
    "\n",
    "E = vca.fit(dataset.X.numpy())\n",
    "endmember_init = torch.from_numpy(vca.endmembers()).float()\n",
    "# endmember_init = (endmember_init / endmember_init.max(dim=1, keepdim=True)[0]) * .8\n",
    "\n",
    "logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))\n",
    "    \n",
    "plt.plot(endmember_init.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pysptools import eea\n",
    "# n_endmembers = dataset.n_endmembers + 0\n",
    "\n",
    "# ee = eea.NFINDR()\n",
    "# endmember = ee.extract(dataset.image().numpy(), n_endmembers)\n",
    "# endmember_init = torch.from_numpy(endmember).float()\n",
    "\n",
    "# # endmember_init = (endmember_init / endmember_init.max(dim=1, keepdim=True)[0]) * .8\n",
    "\n",
    "# logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))\n",
    "\n",
    "# with plt.style.context((\"seaborn-colorblind\")):\n",
    "#     plt.plot(endmember_init.T)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = dataset.n_bands\n",
    "model = ContrastiveUnmixing(n_bands, n_endmembers, endmember_init=logit_endmember_init)\n",
    "train(model, n_endmembers, dataset, n_batchs=80, n_epochs=25, lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = UnmixingLoss()\n",
    "entropy_reg  = NormalizedEntropy(S=n_endmembers)\n",
    "volume_reg = SimplexVolumeLoss(dataset[:], n_endmembers)\n",
    "similarity_reg = SimilarityLoss(n_endmembers, temperature=.1, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = dataset.X\n",
    "\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "reconstruc = model(_X)\n",
    "with torch.no_grad():\n",
    "    print(criterion(reconstruc, _X).cpu(), entropy_reg(model.A).cpu(), volume_reg(sigmoid(model.ebk)).cpu(),\n",
    "         similarity_reg(sigmoid(model.ebk)).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    plt.plot(_X[1020].cpu(), label='original')\n",
    "    plt.plot(reconstruc[1020].cpu().detach(), label='reconstructed')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(map(lambda x: f'$E_{x}$', range(1, n_endmembers+1)))\n",
    "ebk = sigmoid(model.ebk).detach().cpu()\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    plt.plot(ebk.T, label=labels)\n",
    "    plt.ylabel('Reflectance', fontsize='x-large')\n",
    "    plt.xlabel('Bands', fontsize='x-large')\n",
    "    #legend background white\n",
    "    plt.legend(fontsize='x-large', facecolor='white')\n",
    "    plt.xticks(fontsize='x-large')\n",
    "    plt.yticks(fontsize='x-large')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()\n",
    "# fig.savefig(os.path.join(config.IMG_PATH, dataset_name, 'CLHU_Endmembers.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering the endmembers\n",
    "endmembers = dataset.endmembers().detach().cpu()\n",
    "from HySpecLab.metrics import sad\n",
    "sad_result = sad(ebk, endmembers)\n",
    "print(sad_result)\n",
    "idx = torch.argmin(sad_result, dim=1) # Index for reordering the ground truth\n",
    "print(idx)\n",
    "\n",
    "# reorder the endmembers\n",
    "endmembers = endmembers[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(map(lambda x: f'$E_{x}$', range(1, len(dataset.endmembers())+1)))\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    plt.plot(endmembers.T, label=labels)\n",
    "    plt.ylabel('Reflectance', fontsize='x-large')\n",
    "    plt.xlabel('Bands', fontsize='x-large')\n",
    "    plt.legend(fontsize='x-large', facecolor='white')\n",
    "    plt.xticks(fontsize='x-large')\n",
    "    plt.yticks(fontsize='x-large')\n",
    "    # plt.title('Ground Truth', fontsize='x-large')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# fig.savefig(os.path.join(config.IMG_PATH, dataset_name, 'Endmembers_GT.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "test = softmax(model.A.detach(), dim=1).cpu().numpy().reshape(100, 100, n_endmembers)\n",
    "labels = list(map(lambda x: f'$E_{x}$', range(1, n_endmembers+1)))\n",
    "\n",
    "# with plt.style.context((\"seaborn-colorblind\")):\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "for i in range(n_endmembers):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    # plt.imshow(test[:,:,i].T, vmin=0, vmax=softmax(model.A, dim=1).max(), cmap='viridis')\n",
    "    plt.imshow(test[:,:,i].T, cmap='viridis')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(labels[i], fontsize='x-large')\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(os.path.join(config.IMG_PATH, dataset_name, 'CLHU_Abundance.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebk = sigmoid(model.ebk).detach().cpu()\n",
    "endmembers = dataset.endmembers()\n",
    "\n",
    "from HySpecLab.metrics import sad\n",
    "sad_result = sad(ebk, endmembers)\n",
    "print(sad_result)\n",
    "idx = torch.argmin(sad_result, dim=1) # Index for reordering the ground truth\n",
    "print(idx)\n",
    "\n",
    "from torch.nn.functional import mse_loss\n",
    "def rmse(x: torch.Tensor, y: torch.Tensor):\n",
    "    return torch.sqrt(mse_loss(x, y, reduction='none').mean(dim=1))\n",
    "\n",
    "abundance = softmax(model.A.detach(), dim=1).cpu().reshape(100, 100, n_endmembers)\n",
    "abundance = abundance.permute(2,0,1)\n",
    "abundance_gt = dataset.abundance()[:,:,idx].permute(2,0,1) # Reorder the ground truth\n",
    "endmember_gt = dataset.endmembers()[idx, :]\n",
    "\n",
    "rmse_result = rmse(abundance.flatten(1), abundance_gt.flatten(1))\n",
    "print(rmse_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "test = rmse(abundance[1,:,:], abundance_gt[1,:,:])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(abundance[3,:,:].T, cmap='viridis')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(abundance_gt[3,:,:].T, cmap='viridis')\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,2))\n",
    "for i in range(dataset.n_endmembers):\n",
    "    plt.subplot(1,dataset.n_endmembers,i+1)\n",
    "    plt.imshow(abundance_gt[i,:,:].T, cmap='viridis')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.colorbar()\n",
    "    \n",
    "plt.show()\n",
    "# fig.savefig(os.path.join(config.IMG_PATH, dataset_name, 'Abundance_GT.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Informatic-Theoretical view on CLHU\n",
    "\n",
    "$\\begin{align}\n",
    "I(\\mathcal{X}; \\mathcal{Z}) + \\lambda R(\\mathcal{Z}) \\\\\n",
    "% \\frac{\\partial \\mathcal{L}}{\\partial \\mathcal{H}} &= \\frac{\\partial \\mathcal{L}}{\\partial \\mathcal{U}} \\frac{\\partial \\mathcal{U}}{\\partial \\mathcal{H}} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = dataset.abundance().permute(2,0,1)\n",
    "\n",
    "#pick up the 32 first samples with the highest abundance per endmember\n",
    "test_idx = torch.zeros(n_endmembers, 32, dtype=torch.long)\n",
    "for i in range(n_endmembers):\n",
    "    _, test_idx[i,:] = torch.topk(gt[i,:,:].flatten(), 32)\n",
    "\n",
    "# plot it\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    for i in range(n_endmembers):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        plt.plot(dataset[test_idx[i]].T)\n",
    "        plt.title(f'$E_{i+1}$', fontsize='x-large')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sepparate train samples and test samples\n",
    "train_idx = torch.ones_like(gt[0,:,:].flatten(), dtype=torch.bool)\n",
    "train_idx[test_idx] = False\n",
    "train_idx = torch.where(train_idx)[0]\n",
    "\n",
    "# create a subset of the dataset\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_idx.flatten())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.functional import softmax, normalize\n",
    "\n",
    "from HySpecLab.unmixing.mixture import lmm\n",
    "from HySpecLab.unmixing.utils import slide\n",
    "\n",
    "from IPDL import MatrixEstimator\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.functional import softmax, normalize\n",
    "\n",
    "class ContrastiveUnmixing(nn.Module):\n",
    "    def __init__(self, n_bands, n_endmembers, encode_layers=[512, 128, 32], endmember_init=None) -> None:\n",
    "        super(ContrastiveUnmixing, self).__init__()      \n",
    "        encode_layers = [n_bands] + encode_layers\n",
    "        \n",
    "        # Encoder\n",
    "        encoder = []\n",
    "        for idx, test in enumerate(slide(encode_layers, 2)):\n",
    "            encoder.append(self.__encode_layer(*test, dropout=True if idx < len(encode_layers)-2 else False))\n",
    "        self.encoder = nn.Sequential(*encoder)\n",
    "\n",
    "        # Endmember bank\n",
    "        self.ebk = Parameter(torch.randn(n_endmembers, n_bands))\n",
    "        if endmember_init is not None:\n",
    "            self.ebk.data = endmember_init\n",
    "\n",
    "        # Projection layer\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(encode_layers[-1], n_bands, bias=False),\n",
    "            MatrixEstimator(.1)\n",
    "        )\n",
    "        \n",
    "        # Abundance matrix\n",
    "        self.A = None\n",
    "        self.A_matrix_estimator = MatrixEstimator(.1)\n",
    "        self.matrix_estimator = MatrixEstimator(.1)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        r0 = self.projection( self.encoder(input) )\n",
    "\n",
    "        self.A = self.A_matrix_estimator(self.__similarity(r0))\n",
    "        out = lmm(softmax(self.A, dim=1), torch.sigmoid(self.ebk))\n",
    "        return self.matrix_estimator(out)\n",
    "        # return lmm(softmax(self.A, dim=1), torch.sigmoid(self.ebk))\n",
    "\n",
    "    def __encode_layer(self, in_features, out_features, dropout=False):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            MatrixEstimator(.1),\n",
    "            *[nn.ReLU(), nn.Dropout(0.5)] if dropout else [nn.Identity()]\n",
    "        )\n",
    "\n",
    "    def __similarity(self, X: torch.Tensor, temperature=1e-1) -> torch.Tensor:\n",
    "        '''\n",
    "            Cosine similarity between input and endmember bank.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "                x: torch.Tensor, shape=(batch_size, n_bands)\n",
    "                    input tensor.\n",
    "                \n",
    "                temperature: float, default=1e-1\n",
    "                    temperature parameter for contrastive learning.\n",
    "                \n",
    "        '''\n",
    "        bs, n_bands = X.shape\n",
    "        X = normalize(X, dim=1)\n",
    "\n",
    "        normalize_ebk = normalize(self.ebk.detach(), dim=1).expand(bs, -1, -1)\n",
    "        cos = torch.bmm(X.view(bs, 1, n_bands), torch.transpose(normalize_ebk, 1, 2)).squeeze()\n",
    "        return (1 - torch.pow(cos, 2))/temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.metrics import UnmixingLoss, NormalizedEntropy\n",
    "from HySpecLab.metrics.regularization import SimplexVolumeLoss, SimilarityLoss\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPDL import AutoEncoderInformationPlane\n",
    "from IPDL.optim import SilvermanOptimizer\n",
    "\n",
    "def define_Ax(X: torch.Tensor, gamma:float=.8) -> torch.Tensor:\n",
    "    from functools import reduce\n",
    "    from IPDL.functional import matrix_estimator\n",
    "\n",
    "    n = X.size(0)\n",
    "    d = X.size(1) if len(X.shape) == 2 else reduce(lambda x, y: x*y, X.shape[1:])\n",
    "    sigma = gamma * n ** (-1 / (4+d)) * np.sqrt(d) \n",
    "\n",
    "    _, Ax = matrix_estimator(X, sigma=sigma)\n",
    "    return Ax\n",
    "\n",
    "from HySpecLab.metrics import UnmixingLoss, NormalizedEntropy\n",
    "from HySpecLab.metrics.regularization import SimplexVolumeLoss, SimilarityLoss\n",
    "\n",
    "# from HySpecLab.unmixing import ContrastiveUnmixing\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def train(model:nn.Module, n_endmembers:int, train_dataset:Dataset, test_dataset:Dataset, n_batchs:int = 64, n_epochs:int = 100, lr=1e-3):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = UnmixingLoss()\n",
    "    entropy_reg  = NormalizedEntropy(S=n_endmembers)    \n",
    "    volume_reg = SimplexVolumeLoss(train_dataset[:], n_endmembers).to(device)\n",
    "    similarity_reg = SimilarityLoss(n_endmembers, temperature=.1, reduction='mean')\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(train_dataset, batch_size=int(len(train_dataset)/n_batchs), shuffle=True)\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % -1},\n",
    "        )\n",
    "\n",
    "    entropy_weight = 1e-1\n",
    "    simplex_weight = 1e-4\n",
    "    similarity_weight = 1e-1\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    #IPDL\n",
    "    matrix_optimizer = SilvermanOptimizer(model, gamma=1e-2, normalize_dim=True)\n",
    "    ip = AutoEncoderInformationPlane(model)\n",
    "    Ax = define_Ax(test_dataset[:], gamma=1e-2)\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        epoch_loss = 0.\n",
    "        model.train()\n",
    "        for i, (x) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y = model(x)\n",
    "            loss = criterion(y, x) + entropy_weight*entropy_reg(model.A) + simplex_weight*volume_reg(sigmoid(model.ebk)) + similarity_weight*similarity_reg(model.ebk)\n",
    "            epoch_loss += loss.detach().item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            x = test_dataset[:].to(device)\n",
    "            y = model(x)\n",
    "            test_loss = criterion(y, x)\n",
    "            \n",
    "        epoch_iterator.set_postfix(tls=\"%.4f\" % (epoch_loss/(i+1)), vls=\"%.4f\" % test_loss.item())\n",
    "\n",
    "        if epoch == 0: # Solo necesario una vez\n",
    "            matrix_optimizer.step()\n",
    "\n",
    "        _, _ = ip.computeMutualInformation(Ax.to(device))\n",
    "\n",
    "    return ip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysptools import eea\n",
    "n_endmembers = dataset.n_endmembers + 0\n",
    "\n",
    "ee = eea.NFINDR()\n",
    "endmember = ee.extract(dataset.image().numpy(), n_endmembers)\n",
    "endmember_init = torch.from_numpy(endmember).float()\n",
    "logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))\n",
    "\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    plt.plot(endmember_init.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContrastiveUnmixing(dataset.n_bands, n_endmembers, endmember_init=logit_endmember_init)\n",
    "ip = train(model, n_endmembers, train_dataset, test_dataset, n_batchs=32, n_epochs=100, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPDL import MatrixBasedRenyisEntropy\n",
    "\n",
    "Ax = define_Ax(test_dataset[:], gamma=1e-2)\n",
    "\n",
    "with plt.style.context('seaborn-colorblind'):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    reference = MatrixBasedRenyisEntropy.entropy(Ax).cpu()\n",
    "    ax.set(xlim=(0, reference), ylim=(0, reference))\n",
    "    axins = ax.inset_axes([1.15, 0., 1, 1])\n",
    "\n",
    "    Ixt, Ity = ip.getMutualInformation(moving_average_n=10)\n",
    "\n",
    "    for idx, current_Ixt in enumerate(Ixt):\n",
    "        current_Ity = Ity[idx]\n",
    "        ax.scatter(current_Ixt, current_Ity, label=\"layer {}\".format(idx+1))\n",
    "        ax.plot(current_Ixt, current_Ity)\n",
    "\n",
    "        axins.scatter(current_Ixt, current_Ity, label=\"layer {}\".format(idx+1))\n",
    "        axins.plot(current_Ixt, current_Ity)\n",
    "\n",
    "    ax.indicate_inset_zoom(axins, edgecolor=\"black\", linewidth=1.5)\n",
    " \n",
    "ax.set_xlabel(\"$\\mathcal{I}(X;T)$\", fontsize='xx-large')\n",
    "ax.set_ylabel(\"$\\mathcal{I}(T;Y)$\", fontsize='xx-large')\n",
    "\n",
    "axins.set_xlabel(\"$\\mathcal{I}(X;T)$\", fontsize='xx-large')\n",
    "axins.set_ylabel(\"$\\mathcal{I}(T;Y)$\", fontsize='xx-large')\n",
    "axins.legend(fontsize='x-large')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, linestyle='dashed', label='reference')\n",
    "ax.legend(fontsize='x-large')\n",
    "\n",
    "#ticks bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize='x-large')\n",
    "axins.tick_params(axis='both', which='major', labelsize='x-large')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(os.path.join(config.IMG_PATH, dataset_name,'CLHU_ip_end{}.pdf'.format(n_endmembers)), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebk = model.ebk.detach().cpu()\n",
    "with plt.style.context((\"seaborn\")):\n",
    "    with plt.style.context((\"seaborn-colorblind\")):\n",
    "        plt.plot(torch.sigmoid(ebk).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_m = model.encoder[-1][-2].x.cpu()\n",
    "model = model.cpu()\n",
    "# z_m = model.encoder[-1][-2]\n",
    "\n",
    "def similarity_loss(z_m, model, temperature=.1):\n",
    "\n",
    "    X = model.projection(z_m)\n",
    "    bs, n_bands = X.shape\n",
    "    X = normalize(X, dim=1)\n",
    "\n",
    "    normalize_ebk = normalize(model.ebk.detach(), dim=1).expand(bs, -1, -1)\n",
    "    cos = torch.bmm(X.view(bs, 1, n_bands), torch.transpose(normalize_ebk, 1, 2)).squeeze()\n",
    "    return (1 - torch.pow(cos, 2))/temperature\n",
    "\n",
    "Abund = similarity_loss(z_m, model).detach()\n",
    "AAbund = define_Ax(Abund, gamma=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPDL.InformationTheory import MatrixBasedRenyisEntropy as renyis\n",
    "Hz = renyis.entropy(model.encoder[-1][-2].get_matrix())\n",
    "Hz_p = renyis.entropy(model.projection[-1].get_matrix())\n",
    "HA = renyis.entropy(AAbund)\n",
    "\n",
    "\n",
    "Izy = renyis.mutualInformation(AAbund, model.encoder[-1][-2].get_matrix())\n",
    "print(Izy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ax = define_Ax(test_dataset[:], gamma=1e-2)\n",
    "Az = model.encoder[-1][-2].get_matrix()\n",
    "\n",
    "Ixz = renyis.mutualInformation(Ax, Az)\n",
    "Ixz\n",
    "\n",
    "Ixa = renyis.mutualInformation(Ax, AAbund)\n",
    "print(Ixz, Ixa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hx = renyis.entropy(Ax)\n",
    "print(Hx)\n",
    "\n",
    "Ha = renyis.entropy(AAbund)\n",
    "print(Ha)\n",
    "\n",
    "Hz = renyis.entropy(Az)\n",
    "print(Hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = Hz_p - HA\n",
    "print(Hz, HA, MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = Hz_p - HA \n",
    "print(Hz_p, HA, MI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La información mutua puede ser negativa. La información mutua es una medida de la dependencia entre dos variables aleatorias, y se define como la reducción en la incertidumbre de una variable cuando se conoce el valor de la otra variable. Si las variables son independientes, la información mutua es cero, mientras que si son dependientes, la información mutua es positiva.\n",
    "\n",
    "Sin embargo, en algunos casos, la información mutua puede ser negativa. Esto ocurre cuando el conocimiento del valor de una variable reduce la incertidumbre sobre otra variable de tal manera que la información obtenida es menor que la información que ya se tenía antes de conocer el valor de la primera variable. En otras palabras, en este caso, el conocimiento de una variable disminuye la incertidumbre de la otra variable, pero la información obtenida no es suficiente para compensar la información que se pierde al conocer la primera variable.\n",
    "\n",
    "La información mutua negativa puede surgir en diversas situaciones, como por ejemplo en la cancelación de ruido en señales, en la detección de anomalías en datos, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aebk = define_Ax(model.ebk.detach().T, gamma=1e-2)\n",
    "renyis.entropy(Aebk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aebk = define_Ax(torch.tensor(endmember).T, gamma=1e-2)\n",
    "renyis.entropy(Aebk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure the MI between the latent space and EBK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebk = model.ebk.detach().cpu()\n",
    "z = model.projection[-1].x.cpu()\n",
    "\n",
    "Aebk = define_Ax(ebk, gamma=1e-2)\n",
    "Az = define_Ax(z, gamma=1e-2)\n",
    "\n",
    "renyis.entropy(Aebk), renyis.entropy(Az)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebk = model.ebk.detach().cpu()\n",
    "z = model.projection[-1].x.cpu()\n",
    "\n",
    "Aebk = define_Ax(ebk.T, gamma=1e-2)\n",
    "Az = define_Ax(z.T, gamma=1e-2)\n",
    "\n",
    "renyis.entropy(Aebk), renyis.entropy(Az), renyis.mutualInformation(Aebk, Az)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 198\n",
    "d_0, d_1 = 4, 128\n",
    "sigma_0 = 5e-2 * n ** (-1 / (4+d_0)) * np.sqrt(d_0) \n",
    "\n",
    "sigma_1 = 5e-2 * n ** (-1 / (4+d_1)) * np.sqrt(d_1)\n",
    "\n",
    "sigma_0, sigma_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean = z.mean(axis=0)\n",
    "ebk_mean = ebk.mean(axis=0)\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "def calc_MI(x, y, bins):\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return mi\n",
    "\n",
    "calc_MI(z_mean.numpy(), ebk_mean.numpy(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute entropy of z_mean and ebk_mean\n",
    "def calc_H(x, bins):\n",
    "    c_x = np.histogram2d(x, x, bins)[0]\n",
    "    H = mutual_info_score(None, None, contingency=c_x)\n",
    "    return H\n",
    "\n",
    "calc_H(z_mean.numpy(), 100), calc_H(ebk_mean.numpy(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info_score(z_mean, ebk_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ebk.mean(axis=0)\n",
    "\n",
    "plt.plot(torch.sigmoid(ebk.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = z.mean(axis=0)\n",
    "\n",
    "plt.plot(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ixt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.matrix_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.rand(10, 1)\n",
    "model.output_estimator(test) == test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.image().numpy()\n",
    "\n",
    "dict = {'X': X, 'n_endmembers': 4}\n",
    "# export to matlab\n",
    "import scipy.io as sio\n",
    "sio.savemat('jasper.mat', dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchStay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41f72df89919d74d3a449896230e7539fdf54ec0bbe47e8a03d934e2ec4dfa40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
