{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_root_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_root_dir not in sys.path:\n",
    "    sys.path.append(project_root_dir)\n",
    "\n",
    "import torch, config\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Informatic-Theoretical view on CLHU\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathcal{I}(X; Z) + \\lambda \\mathcal{R}(Z)\n",
    "\\end{align}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jasper Ridge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import JasperRidge\n",
    "\n",
    "dataset = JasperRidge(config.JasperRidge_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset, separate a test subset, picking up the 32 samples with the highest 'abundance' value per endmember. Im total, the test subset is going to contain 128 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = dataset.abundance().permute(2,0,1)\n",
    "\n",
    "#pick up the 32 first samples with the highest abundance per endmember\n",
    "test_idx = torch.zeros(dataset.n_endmembers, 32, dtype=torch.long)\n",
    "for i in range(dataset.n_endmembers):\n",
    "    _, test_idx[i,:] = torch.topk(gt[i,:,:].flatten(), 32)\n",
    "\n",
    "# sepparate train samples and test samples\n",
    "train_idx = torch.ones_like(gt[0,:,:].flatten(), dtype=torch.bool)\n",
    "train_idx[test_idx] = False\n",
    "train_idx = torch.where(train_idx)[0]\n",
    "\n",
    "# plot it\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    for i in range(dataset.n_endmembers):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        plt.plot(dataset[test_idx[i]].T)\n",
    "        plt.title(f'$E_{i+1}$', fontsize='x-large')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset of the dataset\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_idx.flatten())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLHU Model\n",
    "The CLHU model including the IPDL modules for IP estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.functional import softmax, normalize\n",
    "\n",
    "from HySpecLab.unmixing.mixture import lmm\n",
    "from HySpecLab.unmixing.utils import slide\n",
    "\n",
    "from IPDL import MatrixEstimator\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.functional import softmax, normalize\n",
    "\n",
    "class ContrastiveUnmixing(nn.Module):\n",
    "    def __init__(self, n_bands, n_endmembers, encode_layers=[512, 128, 32], endmember_init=None) -> None:\n",
    "        super(ContrastiveUnmixing, self).__init__()      \n",
    "        encode_layers = [n_bands] + encode_layers\n",
    "        \n",
    "        # Encoder\n",
    "        encoder = []\n",
    "        for idx, test in enumerate(slide(encode_layers, 2)):\n",
    "            encoder.append(self.__encode_layer(*test, dropout=True if idx < len(encode_layers)-2 else False))\n",
    "        self.encoder = nn.Sequential(*encoder)\n",
    "\n",
    "        # Endmember bank\n",
    "        self.ebk = Parameter(torch.randn(n_endmembers, n_bands))\n",
    "        if endmember_init is not None:\n",
    "            self.ebk.data = endmember_init\n",
    "\n",
    "        # Projection layer\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(encode_layers[-1], n_bands, bias=False),\n",
    "            MatrixEstimator(.1)\n",
    "        )\n",
    "        \n",
    "        # Abundance matrix\n",
    "        self.A = None\n",
    "        self.A_matrix_estimator = MatrixEstimator(.1)\n",
    "        self.matrix_estimator = MatrixEstimator(.1)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        r0 = self.projection( self.encoder(input) )\n",
    "\n",
    "        self.A = self.A_matrix_estimator(self.__similarity(r0))\n",
    "        out = lmm(softmax(self.A, dim=1), torch.sigmoid(self.ebk))\n",
    "        return self.matrix_estimator(out)\n",
    "        # return lmm(softmax(self.A, dim=1), torch.sigmoid(self.ebk))\n",
    "\n",
    "    def __encode_layer(self, in_features, out_features, dropout=False):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            MatrixEstimator(.1),\n",
    "            *[nn.ReLU(), nn.Dropout(0.5)] if dropout else [nn.Identity()]\n",
    "        )\n",
    "\n",
    "    def __similarity(self, X: torch.Tensor, temperature=1e-1) -> torch.Tensor:\n",
    "        '''\n",
    "            Cosine similarity between input and endmember bank.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "                x: torch.Tensor, shape=(batch_size, n_bands)\n",
    "                    input tensor.\n",
    "                \n",
    "                temperature: float, default=1e-1\n",
    "                    temperature parameter for contrastive learning.\n",
    "                \n",
    "        '''\n",
    "        bs, n_bands = X.shape\n",
    "        X = normalize(X, dim=1)\n",
    "\n",
    "        normalize_ebk = normalize(self.ebk.detach(), dim=1).expand(bs, -1, -1)\n",
    "        cos = torch.bmm(X.view(bs, 1, n_bands), torch.transpose(normalize_ebk, 1, 2)).squeeze()\n",
    "        return (1 - torch.pow(cos, 2))/temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.metrics import UnmixingLoss, NormalizedEntropy\n",
    "from HySpecLab.metrics.regularization import SimplexVolumeLoss, SimilarityLoss\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPDL import AutoEncoderInformationPlane\n",
    "from IPDL.optim import SilvermanOptimizer\n",
    "\n",
    "def define_Ax(X: torch.Tensor, gamma:float=.8) -> torch.Tensor:\n",
    "    from functools import reduce\n",
    "    from IPDL.functional import matrix_estimator\n",
    "\n",
    "    n = X.size(0)\n",
    "    d = X.size(1) if len(X.shape) == 2 else reduce(lambda x, y: x*y, X.shape[1:])\n",
    "    sigma = gamma * n ** (-1 / (4+d)) * np.sqrt(d) \n",
    "\n",
    "    _, Ax = matrix_estimator(X, sigma=sigma)\n",
    "    return Ax\n",
    "\n",
    "from HySpecLab.metrics import UnmixingLoss, NormalizedEntropy\n",
    "from HySpecLab.metrics.regularization import SimplexVolumeLoss, SimilarityLoss\n",
    "\n",
    "# from HySpecLab.unmixing import ContrastiveUnmixing\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def train(model:nn.Module, n_endmembers:int, train_dataset:Dataset, test_dataset:Dataset, n_batchs:int = 64, n_epochs:int = 100, lr=1e-3):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = UnmixingLoss()\n",
    "    entropy_reg  = NormalizedEntropy(S=n_endmembers)    \n",
    "    volume_reg = SimplexVolumeLoss(train_dataset[:], n_endmembers).to(device)\n",
    "    similarity_reg = SimilarityLoss(n_endmembers, temperature=.1, reduction='mean')\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(train_dataset, batch_size=int(len(train_dataset)/n_batchs), shuffle=True)\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % -1},\n",
    "        )\n",
    "\n",
    "    entropy_weight = 1e-1\n",
    "    simplex_weight = 1e-4\n",
    "    similarity_weight = 1e-1\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    #IPDL\n",
    "    matrix_optimizer = SilvermanOptimizer(model, gamma=1e-2, normalize_dim=True)\n",
    "    ip = AutoEncoderInformationPlane(model)\n",
    "    Ax = define_Ax(test_dataset[:], gamma=1e-2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        x = test_dataset[:].to(device)\n",
    "        y = model(x)\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        epoch_loss = 0.\n",
    "        model.train()\n",
    "        for i, (x) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y = model(x)\n",
    "            loss = criterion(y, x) + entropy_weight*entropy_reg(model.A) + simplex_weight*volume_reg(sigmoid(model.ebk)) + similarity_weight*similarity_reg(model.ebk)\n",
    "            epoch_loss += loss.detach().item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            x = test_dataset[:].to(device)\n",
    "            y = model(x)\n",
    "            test_loss = criterion(y, x)\n",
    "            \n",
    "        epoch_iterator.set_postfix(tls=\"%.4f\" % (epoch_loss/(i+1)), vls=\"%.4f\" % test_loss.item())\n",
    "\n",
    "        if epoch == 0: # Solo necesario una vez\n",
    "            matrix_optimizer.step()\n",
    "\n",
    "        _, _ = ip.computeMutualInformation(Ax.to(device))\n",
    "\n",
    "    return ip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endmember estimation for initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysptools import eea\n",
    "n_endmembers = dataset.n_endmembers + 0\n",
    "\n",
    "ee = eea.NFINDR()\n",
    "endmember = ee.extract(dataset.image().numpy(), n_endmembers)\n",
    "endmember_init = torch.from_numpy(endmember).float()\n",
    "logit_endmember_init = torch.log((endmember_init / (1-endmember_init) + 1e-12))\n",
    "\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    plt.plot(endmember_init.T)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContrastiveUnmixing(dataset.n_bands, n_endmembers, endmember_init=logit_endmember_init)\n",
    "ip = train(model, n_endmembers, train_dataset, test_dataset, n_batchs=80, n_epochs=25, lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Plane estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPDL import MatrixBasedRenyisEntropy\n",
    "\n",
    "Ax = define_Ax(test_dataset[:], gamma=1e-2)\n",
    "\n",
    "with plt.style.context('seaborn-colorblind'):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    reference = MatrixBasedRenyisEntropy.entropy(Ax).cpu()\n",
    "    ax.set(xlim=(0, reference), ylim=(0, reference))\n",
    "    axins = ax.inset_axes([1.15, 0., 1, 1])\n",
    "\n",
    "    Ixt, Ity = ip.getMutualInformation(moving_average_n=5)\n",
    "\n",
    "    for idx, current_Ixt in enumerate(Ixt):\n",
    "        current_Ity = Ity[idx]\n",
    "        ax.scatter(current_Ixt, current_Ity, label=\"layer {}\".format(idx+1))\n",
    "        ax.plot(current_Ixt, current_Ity)\n",
    "\n",
    "        axins.scatter(current_Ixt, current_Ity, label=\"layer {}\".format(idx+1))\n",
    "        axins.plot(current_Ixt, current_Ity)\n",
    "\n",
    "    ax.indicate_inset_zoom(axins, edgecolor=\"black\", linewidth=1.5)\n",
    " \n",
    "ax.set_xlabel(\"$\\mathcal{I}(X;T)$\", fontsize='xx-large')\n",
    "ax.set_ylabel(\"$\\mathcal{I}(T;Y)$\", fontsize='xx-large')\n",
    "\n",
    "axins.set_xlabel(\"$\\mathcal{I}(X;T)$\", fontsize='xx-large')\n",
    "axins.set_ylabel(\"$\\mathcal{I}(T;Y)$\", fontsize='xx-large')\n",
    "axins.legend(fontsize='x-large')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, linestyle='dashed', label='reference')\n",
    "ax.legend(fontsize='x-large')\n",
    "\n",
    "#ticks bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize='x-large')\n",
    "axins.tick_params(axis='both', which='major', labelsize='x-large')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(os.path.join(config.IMG_PATH, dataset_name,'CLHU_ip_end{}.pdf'.format(n_endmembers)), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(map(lambda x: f'$E_{x}$', range(1, n_endmembers+1)))\n",
    "ebk = torch.sigmoid(model.ebk.detach()).cpu()\n",
    "\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    plt.plot(ebk.T, label=labels)\n",
    "    plt.ylabel('Reflectance', fontsize='x-large')\n",
    "    plt.xlabel('Bands', fontsize='x-large')\n",
    " \n",
    "    plt.legend(fontsize='x-large')\n",
    "    plt.xticks(fontsize='x-large')\n",
    "    plt.yticks(fontsize='x-large')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "_ = model(dataset.X.cuda())\n",
    "\n",
    "test = softmax(model.A.detach(), dim=1).cpu().numpy().reshape(100, 100, n_endmembers)\n",
    "labels = list(map(lambda x: f'$E_{x}$', range(1, n_endmembers+1)))\n",
    "\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "for i in range(n_endmembers):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    # plt.imshow(test[:,:,i].T, vmin=0, vmax=softmax(model.A, dim=1).max(), cmap='viridis')\n",
    "    plt.imshow(test[:,:,i].T, cmap='viridis')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(labels[i], fontsize='x-large')\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_m = model.encoder[-1][-2].x.cpu()\n",
    "model = model.cpu()\n",
    "# z_m = model.encoder[-1][-2]\n",
    "\n",
    "def similarity_loss(z_m, model, temperature=.5):\n",
    "\n",
    "    X = model.projection(z_m)\n",
    "    bs, n_bands = X.shape\n",
    "    X = normalize(X, dim=1)\n",
    "\n",
    "    normalize_ebk = normalize(model.ebk.detach(), dim=1).expand(bs, -1, -1)\n",
    "    cos = torch.bmm(X.view(bs, 1, n_bands), torch.transpose(normalize_ebk, 1, 2)).squeeze()\n",
    "    return (1 - torch.pow(cos, 2))/temperature\n",
    "\n",
    "Abund = similarity_loss(z_m, model).detach()\n",
    "AAbund = define_Ax(Abund, gamma=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPDL.InformationTheory import MatrixBasedRenyisEntropy as renyis\n",
    "Hz = renyis.entropy(model.encoder[-1][-2].get_matrix())\n",
    "Hz_p = renyis.entropy(model.projection[-1].get_matrix())\n",
    "# HA = renyis.entropy(AAbund)\n",
    "\n",
    "\n",
    "# Izy = renyis.mutualInformation(AAbund, model.encoder[-1][-2].get_matrix())\n",
    "# print(Izy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ax = define_Ax(test_dataset[:], gamma=1e-2)\n",
    "Az = model.encoder[-1][-2].get_matrix()\n",
    "\n",
    "Ixz = renyis.mutualInformation(Ax, Az)\n",
    "Ixz\n",
    "\n",
    "Ixa = renyis.mutualInformation(Ax, AAbund)\n",
    "print(Ixz, Ixa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hx = renyis.entropy(Ax)\n",
    "print(Hx)\n",
    "\n",
    "Ha = renyis.entropy(AAbund)\n",
    "print(Ha)\n",
    "\n",
    "Hz = renyis.entropy(Az)\n",
    "print(Hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = Hz - HA\n",
    "print(Hz, HA, MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = Hz_p - HA \n",
    "print(Hz_p, HA, MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renyis.mutualInformation(Az, AAbund)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = model.encoder[1][-3].get_matrix()\n",
    "\n",
    "renyis.entropy(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renyis.entropy(Az)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchStay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41f72df89919d74d3a449896230e7539fdf54ec0bbe47e8a03d934e2ec4dfa40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
