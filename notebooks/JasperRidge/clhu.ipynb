{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_root_dir = os.path.join(os.getcwd(),'../..')\n",
    "if project_root_dir not in sys.path:\n",
    "    sys.path.append(project_root_dir)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "import config\n",
    "\n",
    "from utils import show_abundance, plot_endmembers\n",
    "# from dataset import JasperRidgeDataset\n",
    "from HySpecLab.metrics import rmse, sad\n",
    "from scipy import io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import JasperRidge\n",
    "\n",
    "dataset = JasperRidge(config.JasperRidge_PATH)\n",
    "dataset_name = 'JasperRidge'\n",
    "wv = np.array(dataset.wv, dtype=np.uint)\n",
    "\n",
    "result_path = os.path.join(config.RESULTS_PATH, 'jasperRidge')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_endmembers, show_abundance\n",
    "fig = plot_endmembers(dataset.endmembers(), wv, ticks_range=(0, 1), n_ticks=5)\n",
    "plt.show(fig)\n",
    "fig.savefig(os.path.join(result_path, 'imgs/M_ref.pdf'), bbox_inches='tight')\n",
    "\n",
    "fig = show_abundance(dataset.abundance())\n",
    "plt.show(fig)\n",
    "fig.savefig(os.path.join(result_path, 'imgs/A_ref.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.metrics import sad\n",
    "\n",
    "def sort_endmember(endmembers, gt):\n",
    "    sad_result = sad(endmembers, gt)\n",
    "    e_idx = torch.argmin(sad_result, dim=0) # Index for reordering the ground truth\n",
    "    return e_idx, sad_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.eea import VCA\n",
    "\n",
    "n_endmembers = dataset.n_endmembers\n",
    "   \n",
    "vca = VCA(n_endmembers, snr_input=-1, random_state=25)\n",
    "vca.fit(dataset.X.numpy())\n",
    "endmembers = torch.from_numpy(vca.endmembers()).float()\n",
    "e_idx, sad_result = sort_endmember(endmembers, dataset.endmembers())\n",
    "\n",
    "vca_endmember_init = endmembers[e_idx]\n",
    "vca_logit_endmember_init = torch.log((vca_endmember_init + 1e-12) / ((1-vca_endmember_init) + 1e-12))\n",
    "\n",
    "fig = plot_endmembers(vca_endmember_init, wv, ticks_range=(0, 1))\n",
    "plt.show(fig)\n",
    "\n",
    "fig.savefig(os.path.join(result_path, 'imgs/M_vca.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_endmembers\n",
    "from pysptools import eea\n",
    "n_endmembers = dataset.n_endmembers\n",
    "\n",
    "ee = eea.NFINDR()\n",
    "endmember = torch.from_numpy(ee.extract(dataset.image(), n_endmembers)).float()\n",
    "\n",
    "e_idx, _ = sort_endmember(endmember, dataset.endmembers())\n",
    "nfindr_endmember_init = endmember[e_idx]\n",
    "nfindr_logit_endmember_init = torch.log((nfindr_endmember_init + 1e-12) / ((1-vca_endmember_init) + 1e-12))\n",
    "\n",
    "fig = plot_endmembers(nfindr_endmember_init, wv, ticks_range=(0, 1))\n",
    "plt.show(fig)\n",
    "fig.savefig(os.path.join(result_path, 'imgs/M_nfindr.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_endmembers(dataset.endmembers() / dataset.endmembers().max(), wv, ticks_range=(0, 1), endmember_estimation=[nfindr_endmember_init, vca_endmember_init], ee_labels=['Ground Truth', 'N-FINDR', 'VCA'])\n",
    "plt.show(fig)\n",
    "# fig.savefig(os.path.join(result_path, 'imgs/M_estimation.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endmember_init_method = 'nfindr'\n",
    "# endmember_init = nfindr_endmember_init\n",
    "# logit_endmember_init = nfindr_logit_endmember_init\n",
    "\n",
    "endmember_init_method = 'vca'\n",
    "endmember_init = vca_endmember_init\n",
    "logit_endmember_init = vca_logit_endmember_init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train \n",
    "from HySpecLab.unmixing import ContrastiveUnmixing\n",
    "\n",
    "n_bands = dataset.n_bands\n",
    "model = ContrastiveUnmixing(n_bands, n_endmembers, endmember_init=logit_endmember_init, sigma_sparsity=.25)\n",
    "_ = model(dataset.X)\n",
    "print(model.sparse_gate.regularize())\n",
    "# train(model, n_endmembers, dataset, n_batchs=50, n_epochs=50, lr=1e-3, simplex_weight=5e-3)\n",
    "train(model, n_endmembers, dataset, n_batchs=100, n_epochs=100, lr=1e-3, similarity_weight=1, sparse_weight=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ContrastiveUnmixing(n_bands, n_endmembers, endmember_init=logit_endmember_init, sigma_sparsity=.1)\n",
    "model.eval()\n",
    "z = model.encoder(dataset.X.cuda())\n",
    "model.sparse_gate(z).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_ = model(dataset.X.cuda())\n",
    "print(model.sparse_gate.variational_parameter().flatten())\n",
    "print(model.sparse_gate.variational_parameter().flatten().mean())\n",
    "print(model.sparse_gate.variational_parameter().flatten().min())\n",
    "print(model.sparse_gate.regularize())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(result_path, f'clhu/weights/clhu_{endmember_init_method}.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from HySpecLab.unmixing import ContrastiveUnmixing\n",
    "\n",
    "# # # load model\n",
    "# model = ContrastiveUnmixing(dataset.n_bands, dataset.n_endmembers)\n",
    "\n",
    "# model.load_state_dict(torch.load(os.path.join(result_path, 'clhu/weights/clhu.pth')))\n",
    "# model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.metrics.regularization import SimplexVolumeLoss, SimilarityLoss\n",
    "from HySpecLab.metrics import UnmixingLoss, NormalizedEntropy\n",
    "\n",
    "criterion = UnmixingLoss()\n",
    "entropy_reg  = NormalizedEntropy(S=dataset.n_endmembers)\n",
    "volume_reg = SimplexVolumeLoss(dataset[:], dataset.n_endmembers)\n",
    "similarity_reg = SimilarityLoss(dataset.n_endmembers, temperature=.1, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sigmoid \n",
    "_X = dataset.X\n",
    "\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "reconstruc = model(_X)\n",
    "with torch.no_grad():\n",
    "    print(criterion(reconstruc, _X).cpu(), entropy_reg(model.A).cpu(), volume_reg(sigmoid(model.ebk)).cpu(),\n",
    "         similarity_reg(model.ebk).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_reg(endmember_init), similarity_reg(logit_endmember_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebk = torch.sigmoid(model.ebk).detach().cpu()\n",
    "if endmember_init_method == 'vca':\n",
    "    label = 'VCA'\n",
    "else:\n",
    "    label = 'N-FINDR'\n",
    "fig = plot_endmembers(ebk, ticks_range=(0, 1), endmember_estimation=[endmember_init, nfindr_endmember_init], ee_labels=['CLHU',label, 'N-FINDR'])\n",
    "plt.show(fig)\n",
    "# fig.savefig(os.path.join(result_path, 'clhu/imgs/M_clhu.pdf'), bbox_inches='tight')\n",
    "\n",
    "# fig = plot_endmembers(ebk, ticks_range=(0, 1))\n",
    "# plt.show(fig)\n",
    "# fig.savefig(os.path.join(result_path, 'clhu/imgs/M_clhu_2.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_endmembers(ebk, ticks_range=(0, 1), endmember_estimation=[endmember_init], ee_labels=['CLHU',label])\n",
    "plt.show(fig)\n",
    "\n",
    "fig.savefig(os.path.join(result_path, f'clhu/imgs/M_clhu_{endmember_init_method}.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "test = softmax(model.A.detach(), dim=1).cpu().numpy().reshape(dataset.n_row, dataset.n_col, -1)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_abundance(A, transpose=True, labels:list = None, figsize:tuple=(7,5)):\n",
    "    '''\n",
    "        Show abundance maps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            A : 3-D array, shape (n_rows, n_cols, n_endmembers)\n",
    "                Abundance maps.\n",
    "            labels : list, optional\n",
    "                Labels for endmembers. Default is None.\n",
    "            figsize : tuple, optional\n",
    "                Figure size. Default is (7,5).\n",
    "    '''\n",
    "    _, _, n_endmembers = A.shape\n",
    "\n",
    "    if labels is None:\n",
    "        labels = list(map(lambda x: r'$m_{{{}}}$'.format(x), range(1, n_endmembers+1)))\n",
    "        \n",
    "    ticks_formatter = plt.FormatStrFormatter('%.1f')\n",
    "    fig = plt.figure(figsize=(7,5))\n",
    "    for i in range(n_endmembers):\n",
    "        data = A[:,:,i].T if transpose else A[:,:,i]\n",
    "        plt.subplot(3,4,i+1)\n",
    "        plt.imshow(data, cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(labels[i], fontsize='x-large')\n",
    "        cb = plt.colorbar(format=ticks_formatter, ticks=[data.min() + 1e-3, data.max() - 1e-3],\n",
    "                         orientation='horizontal', fraction=0.1, pad=0.03)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = show_abundance(test)\n",
    "plt.show(fig)\n",
    "\n",
    "fig.savefig(os.path.join(result_path, f'clhu/imgs/A_clhu_estimation_{endmember_init_method}.pdf'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = model._sparse.detach().cpu().numpy().reshape(dataset.n_row, dataset.n_col)\n",
    "test = model.sparse_gate.variational_parameter().detach().cpu().numpy().reshape(dataset.n_row, dataset.n_col)\n",
    "test = np.log(test)\n",
    "plt.imshow(test.T, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true = dataset.A @ dataset.endmembers()\n",
    "X_hat = model(dataset.X).detach().cpu()\n",
    "A_hat = torch.softmax(model.A.detach().cpu(), dim=1)\n",
    "M_hat = sigmoid(model.ebk.detach().cpu())\n",
    "\n",
    "real_M_hat = model(M_hat).detach()\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['Method', 'RMSE_X', 'RMSE_A', 'SAD_M'])\n",
    "df['Method'] = ['CLHU']\n",
    "df['RMSE_X'] = [rmse(X_true, X_hat, dim=None).numpy()]\n",
    "df['RMSE_A'] = [rmse(dataset.A, A_hat, dim=None).numpy()]\n",
    "\n",
    "# sad_result = sad(M_hat, dataset.endmembers()).numpy()\n",
    "sad_result = sad(real_M_hat, dataset.endmembers()).numpy()\n",
    "df['SAD_M'] = np.diagonal(sad_result).mean()\n",
    "\n",
    "# df.to_csv(os.path.join(result_path, 'clhu/metrics.csv'), index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.ebk = sigmoid(model.ebk)\n",
    "x_test = sigmoid(model.ebk)\n",
    "m = model.ebk.detach()\n",
    "model.eval()\n",
    "print(x_test.min(), x_test.max())\n",
    "encoder, projection = model.encoder, model.projection\n",
    "from torch.nn.functional import normalize, softmax\n",
    "\n",
    "def __similarity(X: torch.Tensor, ebk, temperature=1e-2) -> torch.Tensor:\n",
    "        '''\n",
    "            Cosine similarity between input and endmember bank.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "                x: torch.Tensor, shape=(batch_size, n_bands)\n",
    "                    input tensor.\n",
    "                \n",
    "                temperature: float, default=1e-1\n",
    "                    temperature parameter for contrastive learning.\n",
    "                \n",
    "        '''\n",
    "        bs, n_bands = X.shape\n",
    "        X = normalize(X, dim=1)\n",
    "\n",
    "        normalize_ebk = normalize(ebk.detach(), dim=1).expand(bs, -1, -1)\n",
    "        cos = torch.bmm(X.view(bs, 1, n_bands), torch.transpose(normalize_ebk, 1, 2)).squeeze()\n",
    "        # return (1 - torch.pow(cos, 2))/temperature\n",
    "        # return (torch.pow(cos, 2))/temperature\n",
    "        return torch.log(torch.pow(cos, 2))\n",
    "\n",
    "# y = projection(encoder(x_test))\n",
    "# print(softmax(__similarity(y, m, temperature=.1).detach(), dim=1))\n",
    "\n",
    "test = model(x_test)\n",
    "\n",
    "plt.plot(test[2].detach().numpy())\n",
    "plt.plot(x_test[2].detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "softmax(model.A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sigmoid\n",
    "def test(model, dataset):\n",
    "    X = dataset.X\n",
    "    model.eval()\n",
    "    model = model.cpu()\n",
    "    \n",
    "    X_true = dataset.A @ dataset.endmembers()\n",
    "    with torch.no_grad():\n",
    "        X_hat = model(dataset.X)\n",
    "        A_hat = torch.softmax(model.A, dim=1)\n",
    "        M_hat = sigmoid(model.ebk) \n",
    "    \n",
    "    rmse_x = rmse(X_true, X_hat, dim=None).numpy()      \n",
    "    rmse_a = rmse(dataset.A, A_hat, dim=None).numpy()\n",
    "    sad_m = np.diagonal(sad(M_hat, dataset.endmembers()).numpy()).mean()\n",
    "    return rmse_x.item(), rmse_a.item(), sad_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train \n",
    "from HySpecLab.unmixing import ContrastiveUnmixing\n",
    "\n",
    "n_bands = dataset.n_bands\n",
    "\n",
    "batch_rmse_x = []\n",
    "batch_rmse_a = []\n",
    "batch_sad_m = []\n",
    "for i in range(10):\n",
    "    model = ContrastiveUnmixing(n_bands, n_endmembers, endmember_init=logit_endmember_init, sigma_sparsity=.25)\n",
    "    train(model, n_endmembers, dataset, n_batchs=100, n_epochs=100, lr=1e-3, similarity_weight=1, sparse_weight=.5)\n",
    "\n",
    "    rmse_x, rmse_a, sad_m = test(model, dataset)\n",
    "    batch_rmse_x.append(rmse_x)\n",
    "    batch_rmse_a.append(rmse_a)\n",
    "    batch_sad_m.append(sad_m)\n",
    "\n",
    "    print(rmse_x, rmse_a, sad_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['RMSE_X', 'RMSE_A', 'SAD_M'])\n",
    "df['RMSE_X'] = batch_rmse_x\n",
    "df['RMSE_A'] = batch_rmse_a\n",
    "df['SAD_M'] = batch_sad_m\n",
    "\n",
    "# extract mean and std\n",
    "df['RMSE_X'].mean(), df['RMSE_X'].std(), df['RMSE_A'].mean(), df['RMSE_A'].std(), df['SAD_M'].mean(), df['SAD_M'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(result_path, 'clhu/metrics_{}_batch.csv'.format(endmember_init_method)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(result_path, 'clhu/metrics_{}_batch.csv'.format(endmember_init_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigmoid(model.ebk).T.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test.T.detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "sad_m = np.diagonal(sad(test, dataset.endmembers()).detach().numpy()).mean()\n",
    "sad_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize   \n",
    "M = logit_endmember_init\n",
    "M_norm = normalize(M, dim=1)\n",
    "\n",
    "sim_matrix = torch.mm(M_norm, M_norm.T) / .1\n",
    "\n",
    "logit = torch.log((sim_matrix).softmax(dim=1))\n",
    "lo2 = (sim_matrix).log_softmax(dim=1)\n",
    "\n",
    "sim_matrix, torch.isclose(logit, lo2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_2 = M.clone()\n",
    "\n",
    "M_2[0] = M_2[2]\n",
    "M_norm = normalize(M_2, dim=1)\n",
    "\n",
    "sim_matrix = torch.mm(M_norm, M_norm.T) / .1\n",
    "\n",
    "logit = torch.log((sim_matrix).softmax(dim=1))\n",
    "lo2 = (sim_matrix).log_softmax(dim=1)\n",
    "\n",
    "sim_matrix, torch.isclose(logit, lo2)\n",
    "\n",
    "# (sim_matrix).softmax(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLHU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
